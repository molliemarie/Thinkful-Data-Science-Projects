{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Thinkful Data Science Course\n",
    "##Unit 4: Predicting the Future; \n",
    "##Lesson 8: Evaluating Classifier Performance\n",
    "\n",
    "###Evaluating Classifier Performance Overview\n",
    "\n",
    "Throughout the unit we've been splitting our data into training, test, and validation sets. Let's take a moment and discuss why this is necesary. By now you can probably see that learning an estimator and testing that estimator's performance on the same data is a methodological mistake. It's like if a professor administered a test with the exact same questions as the practice test. All a student would have to do to get 100% would be to memorize all the solutions to the practice test; they wouldn't acutally have to learn anything. If you test your estimator on the data used to train it, it knows all the answers, and thus can achieve a perfect score, even though it very well could fail to predict any- thing on data it's never seen before. This is called overfitting. Predicting on never-before-seen data is kind of the whole point, so knowing how our estimator performs on data its already seen isn't really useful.\n",
    "\n",
    "Holding out a subset of your data for testing, i.e., excluding a subset of your data from your training set, gives you some never-before-seen data to test your estimator's performance. The scikit-learn library has a train_test_split helper function to randomly split data into training and test sets.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the C setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and we can't make claims about how it will generalize (i.e., how it will perform) on never-before-seen data.\n",
    "\n",
    "To resolve this problem, we can hold out yet another subset of our data for validation. Training proceeds on the training set, evaluation is done on the validation set, and when it seems like we have a good model, we can perform our final evaluation on the test set.\n",
    "\n",
    "####Use the cross_validation.train_test_split() helper function to split the Iris dataset into training and test sets, holding out 40% of the data for testing. \n",
    "How many points do you have in your training set? In your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iris_df = pd.DataFrame()\n",
    "iris_df['sepal_length'] = iris.data[:,0]\n",
    "iris_df['sepal_width'] = iris.data[:,1]\n",
    "iris_df['petal_length'] = iris.data[:,2]\n",
    "iris_df['petal_width'] = iris.data[:,3]\n",
    "iris_df['target'] = iris.target\n",
    "iris_df['target_flower'] = iris.target\n",
    "iris_df['target_flower'].replace(0, 'setosa', inplace = True)\n",
    "iris_df['target_flower'].replace(1, 'versicolor', inplace = True)\n",
    "iris_df['target_flower'].replace(2, 'virginica', inplace = True)\n",
    "iris_df1 = iris_df[iris_df['target_flower']=='setosa']\n",
    "iris_df2 = iris_df[iris_df['target_flower']=='versicolor']\n",
    "iris_df3 = iris_df[iris_df['target_flower']=='virginica']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = iris_df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']].as_matrix()\n",
    "y = iris_df['target'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train,y_test = train_test_split(X, y, test_size=0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.9,  3.2,  4.8,  1.8],\n",
       "       [ 6.7,  3.1,  4.4,  1.4],\n",
       "       [ 5.6,  3. ,  4.1,  1.3],\n",
       "       [ 5.6,  2.8,  4.9,  2. ],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 6.7,  2.5,  5.8,  1.8],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.7,  2.8,  4.1,  1.3],\n",
       "       [ 5.9,  3. ,  5.1,  1.8],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 4.5,  2.3,  1.3,  0.3],\n",
       "       [ 6.2,  2.8,  4.8,  1.8],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 7. ,  3.2,  4.7,  1.4],\n",
       "       [ 5.5,  3.5,  1.3,  0.2],\n",
       "       [ 5.7,  2.8,  4.5,  1.3],\n",
       "       [ 5. ,  2. ,  3.5,  1. ],\n",
       "       [ 5.5,  2.4,  3.7,  1. ],\n",
       "       [ 5. ,  3.6,  1.4,  0.2],\n",
       "       [ 6.4,  2.9,  4.3,  1.3],\n",
       "       [ 6.7,  3. ,  5. ,  1.7],\n",
       "       [ 5.2,  2.7,  3.9,  1.4],\n",
       "       [ 6.4,  2.8,  5.6,  2.1],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 6.7,  3. ,  5.2,  2.3],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 6.3,  2.3,  4.4,  1.3],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 6. ,  2.9,  4.5,  1.5],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 4.4,  3. ,  1.3,  0.2],\n",
       "       [ 5.5,  2.3,  4. ,  1.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 6.9,  3.1,  5.1,  2.3],\n",
       "       [ 6.1,  3. ,  4.9,  1.8],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 6.4,  2.7,  5.3,  1.9],\n",
       "       [ 6.3,  2.5,  5. ,  1.9],\n",
       "       [ 5.4,  3.9,  1.3,  0.4],\n",
       "       [ 6.8,  3. ,  5.5,  2.1],\n",
       "       [ 6.3,  3.4,  5.6,  2.4],\n",
       "       [ 6.3,  2.8,  5.1,  1.5],\n",
       "       [ 5. ,  3.3,  1.4,  0.2],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 6.6,  2.9,  4.6,  1.3],\n",
       "       [ 7.9,  3.8,  6.4,  2. ],\n",
       "       [ 5.7,  3. ,  4.2,  1.2],\n",
       "       [ 4.6,  3.1,  1.5,  0.2],\n",
       "       [ 5.7,  3.8,  1.7,  0.3],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 6.1,  2.8,  4. ,  1.3],\n",
       "       [ 5.1,  2.5,  3. ,  1.1],\n",
       "       [ 7.2,  3. ,  5.8,  1.6],\n",
       "       [ 6. ,  2.2,  4. ,  1. ],\n",
       "       [ 4.8,  3.4,  1.6,  0.2],\n",
       "       [ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 5.1,  3.7,  1.5,  0.4],\n",
       "       [ 5. ,  3.2,  1.2,  0.2],\n",
       "       [ 4.7,  3.2,  1.6,  0.2],\n",
       "       [ 6.8,  2.8,  4.8,  1.4],\n",
       "       [ 6.3,  2.5,  4.9,  1.5],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 6.4,  3.2,  4.5,  1.5],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 4.6,  3.6,  1. ,  0.2],\n",
       "       [ 6.1,  2.6,  5.6,  1.4],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 6. ,  3. ,  4.8,  1.8],\n",
       "       [ 4.6,  3.4,  1.4,  0.3],\n",
       "       [ 4.9,  2.4,  3.3,  1. ],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 5.1,  3.8,  1.6,  0.2],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 5.2,  4.1,  1.5,  0.1],\n",
       "       [ 6.7,  3.1,  4.7,  1.5],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 6.3,  2.7,  4.9,  1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6.2,  2.2,  4.5,  1.5],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5.8,  2.7,  4.1,  1. ],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 5.5,  2.4,  3.8,  1.1],\n",
       "       [ 5.8,  2.7,  3.9,  1.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 4.8,  3.4,  1.9,  0.2],\n",
       "       [ 6.8,  3.2,  5.9,  2.3],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 4.8,  3. ,  1.4,  0.1],\n",
       "       [ 7.7,  3.8,  6.7,  2.2],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 6.9,  3.1,  4.9,  1.5],\n",
       "       [ 6.7,  3.3,  5.7,  2.1],\n",
       "       [ 5.9,  3. ,  4.2,  1.5],\n",
       "       [ 6.1,  2.9,  4.7,  1.4],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 6.7,  3.3,  5.7,  2.5],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 7.6,  3. ,  6.6,  2.1],\n",
       "       [ 7.2,  3.6,  6.1,  2.5],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.4,  3.4,  1.7,  0.2],\n",
       "       [ 5.4,  3.7,  1.5,  0.2],\n",
       "       [ 4.8,  3.1,  1.6,  0.2],\n",
       "       [ 6.5,  3.2,  5.1,  2. ],\n",
       "       [ 6. ,  3.4,  4.5,  1.6],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 5.4,  3.4,  1.5,  0.4],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 5.1,  3.3,  1.7,  0.5],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 5.8,  2.6,  4. ,  1.2],\n",
       "       [ 6.9,  3.1,  5.4,  2.1],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6. ,  2.2,  5. ,  1.5],\n",
       "       [ 5. ,  3.5,  1.6,  0.6],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 7.4,  2.8,  6.1,  1.9],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 5.6,  2.7,  4.2,  1.3],\n",
       "       [ 5.8,  2.7,  5.1,  1.9],\n",
       "       [ 7.7,  3. ,  6.1,  2.3]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 2, 2, 2, 0, 2, 0, 0, 1, 0, 1, 1, 1, 0,\n",
       "       1, 1, 1, 2, 0, 2, 1, 1, 1, 0, 2, 1, 1, 0, 0, 1, 1, 0, 2, 2, 2, 2, 2,\n",
       "       0, 2, 2, 2, 0, 0, 0, 1, 2, 1, 0, 0, 0, 1, 1, 1, 2, 1, 0, 0, 0, 0, 0,\n",
       "       1, 1, 2, 1, 0, 2, 0, 2, 2, 0, 2, 2, 0, 1, 2, 0, 1, 0, 1, 2, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 0, 1, 1, 1, 1, 2, 0, 2, 0, 0, 2, 0, 1, 2, 1, 1, 0, 0, 2, 0,\n",
       "       0, 2, 2, 1, 0, 2, 0, 0, 0, 2, 1, 2, 0, 0, 1, 1, 2, 0, 1, 2, 2, 1, 2,\n",
       "       1, 2, 0, 0, 0, 2, 0, 1, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####How many points do you have in your training set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  90 points in the training set\n"
     ]
    }
   ],
   "source": [
    "print('There are ', len(X_train), 'points in the training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####In your test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60 points in the test set, which is 40.0 % of the data.\n"
     ]
    }
   ],
   "source": [
    "print('There are', len(X_test), 'points in the test set, which is', (len(X_test)/(len(X_test)+len(X_train)))*100, '% of the data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Fit a linear Support Vector Classifier to the training set and evaluate its performance on the test set. \n",
    "\n",
    "What is the score? How does it compare to the score in the Support Vector Machine lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97777777777777775"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "X=X_train\n",
    "y=y_train\n",
    "clf = SVC()\n",
    "clf.fit(X,y)\n",
    "clf.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####What is the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SVC score is 0.977777777778\n"
     ]
    }
   ],
   "source": [
    "print('The SVC score is', clf.score(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####How does it compare to the score in the Support Vector Machine lesson?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###Cross Validation\n",
    "\n",
    "The more data we set aside for testing and validation, the less data we have for training, and this will negatively impact estimator performance. To resolve this problem, we can use cross validation (see lesson 4.1.5) to \"recycle\" data over different folds. In this assignment, we're going to implement 5-fold cross-validation on the Iris dataset to train and test a Support Vector Machine classifier.\n",
    "\n",
    "####Compute the 5-fold cross-validation score of the SVC from the last assignment.\n",
    "\n",
    "####Compute the mean score and the standard deviation of the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "import statsmodels.api as sm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'svc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-fc270f8d7936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'svc'"
     ]
    }
   ],
   "source": [
    "kf = cross_validation.KFold(len(y), n_folds=5)\n",
    "r2 = []\n",
    "mae = []\n",
    "mse = []\n",
    "for train_index, test_index in kf:\n",
    "    model = svm.svc(y_train,X_train)\n",
    "    f = model.fit()\n",
    "    y_pred = f.predict(X_test)\n",
    "    r2.append(r2_score(np.squeeze(np.asarray(y_test)), y_pred))\n",
    "    mse.append(mean_squared_error(np.squeeze(np.asarray(y_test)), y_pred))\n",
    "    mae.append(mean_absolute_error(np.squeeze(np.asarray(y_test)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(y), n_folds=5)\n",
    "r2 = []\n",
    "mae = []\n",
    "mse = []\n",
    "for train_index, test_index in kf:\n",
    "    model = sm.OLS(y_train,X_train)\n",
    "    f = model.fit()\n",
    "    y_pred = f.predict(X_test)\n",
    "    r2.append(r2_score(np.squeeze(np.asarray(y_test)), y_pred))\n",
    "    mse.append(mean_squared_error(np.squeeze(np.asarray(y_test)), y_pred))\n",
    "    mae.append(mean_absolute_error(np.squeeze(np.asarray(y_test)), y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5. ,  3. ,  1.6,  0.2],\n",
       "       [ 5.2,  3.5,  1.5,  0.2],\n",
       "       [ 5.7,  2.9,  4.2,  1.3],\n",
       "       [ 5.7,  4.4,  1.5,  0.4],\n",
       "       [ 4.9,  3. ,  1.4,  0.2],\n",
       "       [ 7.7,  2.6,  6.9,  2.3],\n",
       "       [ 5.6,  2.5,  3.9,  1.1],\n",
       "       [ 5. ,  3.5,  1.3,  0.3],\n",
       "       [ 6.3,  3.3,  4.7,  1.6],\n",
       "       [ 6.2,  2.9,  4.3,  1.3],\n",
       "       [ 5. ,  2.3,  3.3,  1. ],\n",
       "       [ 4.9,  2.5,  4.5,  1.7],\n",
       "       [ 5.1,  3.5,  1.4,  0.2],\n",
       "       [ 5. ,  3.4,  1.6,  0.4],\n",
       "       [ 7.7,  2.8,  6.7,  2. ],\n",
       "       [ 4.3,  3. ,  1.1,  0.1],\n",
       "       [ 6.3,  3.3,  6. ,  2.5],\n",
       "       [ 5.3,  3.7,  1.5,  0.2],\n",
       "       [ 5.1,  3.8,  1.5,  0.3],\n",
       "       [ 4.7,  3.2,  1.3,  0.2],\n",
       "       [ 6.5,  3. ,  5.2,  2. ],\n",
       "       [ 7.2,  3.2,  6. ,  1.8],\n",
       "       [ 7.3,  2.9,  6.3,  1.8],\n",
       "       [ 5.1,  3.8,  1.9,  0.4],\n",
       "       [ 6.6,  3. ,  4.4,  1.4],\n",
       "       [ 6.7,  3.1,  5.6,  2.4],\n",
       "       [ 4.6,  3.2,  1.4,  0.2],\n",
       "       [ 6.5,  2.8,  4.6,  1.5],\n",
       "       [ 6.5,  3. ,  5.5,  1.8],\n",
       "       [ 6. ,  2.7,  5.1,  1.6],\n",
       "       [ 5.1,  3.5,  1.4,  0.3],\n",
       "       [ 5.6,  2.9,  3.6,  1.3],\n",
       "       [ 5.8,  2.8,  5.1,  2.4],\n",
       "       [ 4.4,  2.9,  1.4,  0.2],\n",
       "       [ 6.3,  2.9,  5.6,  1.8],\n",
       "       [ 6.4,  3.2,  5.3,  2.3],\n",
       "       [ 6.5,  3. ,  5.8,  2.2],\n",
       "       [ 5.5,  2.6,  4.4,  1.2],\n",
       "       [ 4.4,  3.2,  1.3,  0.2],\n",
       "       [ 5.7,  2.5,  5. ,  2. ],\n",
       "       [ 6.4,  2.8,  5.6,  2.2],\n",
       "       [ 5.6,  3. ,  4.5,  1.5],\n",
       "       [ 6.2,  3.4,  5.4,  2.3],\n",
       "       [ 5.5,  2.5,  4. ,  1.3],\n",
       "       [ 5.4,  3.9,  1.7,  0.4],\n",
       "       [ 5.5,  4.2,  1.4,  0.2],\n",
       "       [ 6.1,  3. ,  4.6,  1.4],\n",
       "       [ 7.1,  3. ,  5.9,  2.1],\n",
       "       [ 6.9,  3.2,  5.7,  2.3],\n",
       "       [ 6.1,  2.8,  4.7,  1.2],\n",
       "       [ 5.8,  4. ,  1.2,  0.2],\n",
       "       [ 4.9,  3.1,  1.5,  0.1],\n",
       "       [ 5.4,  3. ,  4.5,  1.5],\n",
       "       [ 4.8,  3. ,  1.4,  0.3],\n",
       "       [ 6.4,  3.1,  5.5,  1.8],\n",
       "       [ 5.2,  3.4,  1.4,  0.2],\n",
       "       [ 5.7,  2.6,  3.5,  1. ],\n",
       "       [ 5.1,  3.4,  1.5,  0.2],\n",
       "       [ 5. ,  3.4,  1.5,  0.2],\n",
       "       [ 4.6,  3.6,  1. ,  0.2]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.94809477892140359,\n",
       " 0.94809477892140359,\n",
       " 0.94809477892140359,\n",
       " 0.94809477892140359,\n",
       " 0.94809477892140359]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.14930339544048812,\n",
       " 0.14930339544048812,\n",
       " 0.14930339544048812,\n",
       " 0.14930339544048812,\n",
       " 0.14930339544048812]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.037544776580184777,\n",
       " 0.037544776580184777,\n",
       " 0.037544776580184777,\n",
       " 0.037544776580184777,\n",
       " 0.037544776580184777]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the sklean documentation notes, the default score computed at each cross-validation iteration is the estimator's accuracy. We could tell it to return the F1 score, precision, or recall, instead.\n",
    "\n",
    "How do the accuracy scores compare to the F1 scores for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't handle mix of multiclass and continuous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-d2d83434325c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Accuracy = %f\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/molliepettit/anaconda/lib/python3.4/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/molliepettit/anaconda/lib/python3.4/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         raise ValueError(\"Can't handle mix of {0} and {1}\"\n\u001b[0;32m---> 84\u001b[0;31m                          \"\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't handle mix of multiclass and continuous"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %f\" %(skm.accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Why doesn't the above code work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2,\n",
       "       0, 1, 2, 0, 1, 2, 1, 0, 1, 2, 0, 2, 2, 2, 1, 0, 2, 2, 1, 2, 1, 0, 0,\n",
       "       1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.03939035, -0.093304  ,  1.19345255, -0.0478905 , -0.0828931 ,\n",
       "        2.25476896,  1.01691575, -0.07110942,  1.42301126,  1.16464357,\n",
       "        0.86728198,  1.58655055, -0.10943756,  0.0662745 ,  2.0350162 ,\n",
       "       -0.15204857,  2.26896606, -0.10616891, -0.03005057, -0.0894203 ,\n",
       "        1.75767666,  1.78742809,  1.86074394,  0.1338879 ,  1.20071728,\n",
       "        2.06171432, -0.05081547,  1.32278227,  1.73086084,  1.57108275,\n",
       "       -0.05497586,  1.04047302,  2.02843298, -0.02590029,  1.78151594,\n",
       "        1.95803731,  2.03081521,  1.21864441, -0.05571339,  1.79689653,\n",
       "        1.98894173,  1.39490452,  2.0062485 ,  1.16444398,  0.04462797,\n",
       "       -0.16008255,  1.31163384,  1.9363089 ,  2.01133591,  1.23170891,\n",
       "       -0.24689857, -0.11080025,  1.4173758 , -0.01719577,  1.74128184,\n",
       "       -0.11985856,  0.84092701, -0.08125373, -0.07001809, -0.16355079])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears y_pred is not given as 0, 1, or 2... Perhaps I just need to round?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -0.,  1., -0., -0.,  2.,  1., -0.,  1.,  1.,  1.,  2., -0.,\n",
       "        0.,  2., -0.,  2., -0., -0., -0.,  2.,  2.,  2.,  0.,  1.,  2.,\n",
       "       -0.,  1.,  2.,  2., -0.,  1.,  2., -0.,  2.,  2.,  2.,  1., -0.,\n",
       "        2.,  2.,  1.,  2.,  1.,  0., -0.,  1.,  2.,  2.,  1., -0., -0.,\n",
       "        1., -0.,  2., -0.,  1., -0., -0., -0.])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  1.,  0.,  0.,  2.,  1.,  0.,  1.,  1.,  1.,  2.,  0.,\n",
       "        0.,  2.,  0.,  2.,  0.,  0.,  0.,  2.,  2.,  2.,  0.,  1.,  2.,\n",
       "        0.,  1.,  2.,  2.,  0.,  1.,  2.,  0.,  2.,  2.,  2.,  1.,  0.,\n",
       "        2.,  2.,  1.,  2.,  1.,  0.,  0.,  1.,  2.,  2.,  1.,  0.,  0.,\n",
       "        1.,  0.,  2.,  0.,  1.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = abs(y_pred.round())\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 0, 0, 2, 1, 0, 1, 1, 1, 2, 0, 0, 2, 0, 2, 0, 0, 0, 2, 2, 2,\n",
       "       0, 1, 2, 0, 1, 2, 2, 0, 1, 2, 0, 2, 2, 2, 1, 0, 2, 2, 1, 2, 1, 0, 0,\n",
       "       1, 2, 2, 1, 0, 0, 1, 0, 2, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = P.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished rounding, now find Accuracy, Precision, Recall, F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.983333\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy = %f\" %(skm.accuracy_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision = 0.984167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/molliepettit/anaconda/lib/python3.4/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "print(\"Precision = %f\" %(skm.precision_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall = 0.983333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/molliepettit/anaconda/lib/python3.4/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall = %f\" %(skm.recall_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.983278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/molliepettit/anaconda/lib/python3.4/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "print(\"F1 score = %f\" %(skm.f1_score(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
